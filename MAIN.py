import sysfrom astropy.table import Table, vstack, Column, QTableimport pandasimport globimport numpy as npimport psrqpy as psfrom astropy.coordinates import SkyCoordimport astropy.coordinatesfrom astropy import units as u, constants as cimport pickleimport matplotlib.pyplot as pltimport mathimport osimport cutoutfrom os.path import expanduserimport astropyfrom astropy.coordinates import Anglefrom scipy.optimize import curve_fitimport scipyfrom PIL import Image, ImageDraw, ImageFontimport textwrapdef slopeFunc(x, a, b):    return a * x + bdef racsTableCompute(path_tables, catalog, specific_table="", file_edges = [55,63]):    # Goes through all the components in the racs_catv tables which hold the detected objects in the mosaics    # and creates a table out of them all    racs_tables = []    for type in ['islands', 'components']:        if specific_table == "":            selavy_files = sorted(glob.glob((path_tables + '/*{0}.txt').format(type)))        else:            selavy_files = [(path_tables + '/' + specific_table + '{0}.txt').format(type)]        data = []        for i, file in enumerate(selavy_files):            table = Table.from_pandas(pandas.read_fwf(file, skiprows=[1, ]))            table.add_column(Column(np.array(file[file_edges[0]:file_edges[1]]), name="fileName"))            table.add_column(Column(np.array(type), name="stokesType"))            table.add_column(Column(np.array(catalog), name="type"))            data.append(table)            print('Read {0} lines from {1} ({2}/{3}) \r'.format(len(data[-1]),                                                                file, i, len(selavy_files)))        racs_data = vstack(data)        # these are empty and confuse HDF5        del racs_data['#']        del racs_data['comment']        if specific_table == "":            racs_data.write((catalog + '_{0}.hdf5').format(type), overwrite=True)            print(('Wrote to ' + catalog + '_{0}.hdf5').format(type))        else:            racs_tables.append(racs_data)    return racs_tablesdef getASKAP(separations, racsvPath = "RACS_StokesV_components.hdf5",             racsiPath = "RACS_StokesI_components.hdf5"  ,stokesType ="components"):    max_position_uncertainty = 1 * u.arcsec    columns = ['Name',               'RAJ',               'DECJ',               'RAJD',               'DECJD',               'DM',               'S400',               'S1400',               'S2000',               'ASSOC',               'DIST',               # 'L400',               # 'L1400',               ]    requery = True    if requery:        query = ps.QueryATNF(params=columns)        data = query.table        with open('pulsars.table', 'wb') as pulsarDoc:            pickle.dump(data, pulsarDoc)    else:        with open('pulsars.table', 'rb') as pulsarDoc:            data = pickle.load(pulsarDoc)    # it's faster to make a coordinate object out of floats than strings    coords = SkyCoord(data['RAJD'], data['DECJD'], unit=('deg', 'deg'))    data.add_column(Column(coords, name="skycoord"))    data.add_column(Column(np.array("ATNF"), name="type"))    # let's do some selections on the coordinates and catalog parameters    # we need the cos(Dec) term in here because an error in RA (or any longitude)    # changes near the poles    position_uncertainty = np.sqrt(        (data['RAJD_ERR'].quantity * np.cos(data['DECJD'].quantity)) ** 2 + (data['DECJD_ERR'].quantity) ** 2)    # it's probably not a bad idea to make a histogram of these    plt.clf()    plt.hist(np.log10(position_uncertainty.to(u.arcsec).value), bins=30, cumulative=True)    plt.xlabel('log10(Position uncertainty [arcsec])')    plt.ylabel('Number of pulsars < uncertainty')    plt.plot(np.log10(max_position_uncertainty.to(u.arcsec).value) * np.array([1, 1]),             plt.gca().get_ylim(), 'r--')    plt.title("Log Histrogram of the distance between the closest objects in StokesI and StokesV")    plt.savefig('position_uncertainty.png')    # make sure it's not in a globular cluster    globular_cluster = np.array([('GC' in x["ASSOC"]) or ("J1748-2446" in x["NAME"]) for x in data]) # Name hardcoded as not marked properly in catalog    # by using astropy units,  we can be assured that the position_uncertainty (in deg) and the    # max_position_uncertainty (in arcsec) are handled correctly    good = (position_uncertainty < max_position_uncertainty) & (~np.isnan(position_uncertainty)) & (~globular_cluster)    print('Total number of pulsars: %d' % len(data))    print('Number not in globular cluster and with position uncertainty < %.1f arcsec: %d' % (        max_position_uncertainty.to(u.arcsec).value,        good.sum()))    # now select the ones we like    data = data[good]    coords = coords[good]    position_uncertainty = position_uncertainty[good]    # read in RACS    racsv_data = Table.read(racsvPath)    racsv_coords = SkyCoord(racsv_data['ra_deg_cont'], racsv_data['dec_deg_cont'], unit=('deg', 'deg'))    racsI_data = Table.read(racsiPath)    racsI_data["flux_peak"] = racsI_data["flux_peak"] * u.mJy / u.beam    racsI_data["flux_int"] = racsI_data["flux_int"] * u.mJy    racsI_data["flux_int_err"] = racsI_data["flux_int_err"] * u.mJy    racsI_data["ra_deg_cont"] = racsI_data["ra_deg_cont"] * u.deg    racsI_data["dec_deg_cont"] = racsI_data["dec_deg_cont"] * u.deg    racsI_data["ra_err"] = racsI_data["ra_err"] * u.arcsec    racsI_data["dec_err"] = racsI_data["dec_err"] * u.arcsec    racsI_coords = SkyCoord(racsI_data['ra_deg_cont'], racsI_data['dec_deg_cont'], unit=('deg', 'deg'))    print("matching racsv ({}) and ATNF ({}) skycoords".format(len(racsv_coords),len(coords)))    imatch_racsV, d2_racsV, _ = coords.match_to_catalog_sky(racsv_coords)    print("matching racsi ({}) and ATNF ({}) skycoords".format(len(racsv_coords),len(coords)))    imatch_racsI, d2_racsI, _ = coords.match_to_catalog_sky(racsI_coords)    # it's probably not a bad idea to make a histogram of the separations    # aa cumulative histogram will enable us to judge how many are on each side of the line    # and we will plot log(separation)    # since we want to see a small number total, use log=True for the y-axis too    for i in separations:        max_separation = i["max_separation"]        min_separation = i["min_separation"]        plt.clf()        plt.hist(np.log10(d2_racsV.arcsec), bins=30, cumulative=True, log=True)        plt.xlabel('log10(Separation from pulsar to ' + "stokesV" + ' source [arcsec])')        plt.ylabel('Number of pulsars < separation')        plt.plot(np.log10(max_separation.to(u.arcsec).value) * np.array([1, 1]),                 plt.gca().get_ylim(), 'r--')        plt.savefig('_separation.png')        #print('Number with separation < %.1f arcsec: %d' % (         #   max_separation.to(u.arcsec).value, (d2 < max_separation and d2 > min_separation).sum()))        for j in range(len(data)):            if d2_racsV[j] < max_separation and d2_racsV[j] > min_separation:                print('PSR %s matches component: flux_peak=%.1f mJy, separation=%.1f arcsec' % (                    data[j]['NAME'],                    racsv_data[imatch_racsV[j]]['flux_peak'],                    d2_racsV[j].arcsec))        # Creates a list of both the table of matches and table of pulsars which correspond to those matches        matches_data = [racsv_data[imatch_racsV[(d2_racsV < max_separation) & (d2_racsV > min_separation)]],                        data[(d2_racsV < max_separation) & (d2_racsV > min_separation)], d2_racsV[(d2_racsV < max_separation) & (d2_racsV > min_separation)]]        S888, S888Error = createS888(data)        data.add_column(Column(S888, name="S888Calc"))        data.add_column(Column(S888Error, name="S888_ERR"))        print(data["S400_ERR"])        print(data["S400"])        data["S888Calc"] = data["S888Calc"] * u.mJy        data["S888_ERR"] = data["S888_ERR"] * u.mJy        print(data["S400_ERR"])        print(data["S400"])        sets = [getMatches(d2_racsV,max_separation,min_separation) & getMatches(d2_racsI, max_separation,min_separation),                getMatches(d2_racsV,max_separation,min_separation) & np.logical_not(getMatches(d2_racsI, max_separation, min_separation)),                np.logical_not(getMatches(d2_racsV, max_separation,min_separation)) & getMatches(d2_racsI, max_separation, min_separation),                np.logical_not(getMatches(d2_racsV, max_separation,min_separation)) & np.logical_not(getMatches(d2_racsI, max_separation, min_separation))]        set_names = ["both_match", "only_racsV", "only_racsI", "neither_match"]        for j in range(4):            matches = {"racsi": racsI_data[imatch_racsI[sets[j]]],                       "racsv": racsv_data[imatch_racsV[sets[j]]],                       "ASKAP": data[sets[j]],                       "distanceATNFandV": d2_racsV[sets[j]] * u.deg}            racsI_coords = SkyCoord(matches["racsi"]['ra_deg_cont'], matches["racsi"]['dec_deg_cont'], unit=('deg', 'deg'))            racsV_coords = SkyCoord(matches["racsv"]['ra_deg_cont'], matches["racsv"]['dec_deg_cont'], unit=('deg', 'deg'))            matches["racsv"].add_column(Column("RACS_StokesV", name="type"))            matches["racsv"].add_column(Column("components", name="stokesType"))            matches["racsi"].add_column(Column(racsI_coords, name="skycoord"))            matches["racsv"].add_column(Column(racsV_coords, name="skycoord"))            imatch, d2, _ = racsV_coords.match_to_catalog_sky(racsI_coords)            matches["distanceIandV"] = d2 * u.deg            with open(('{}_{}_{}_ASKAP_matches.table').format(stokesType, i["name"], set_names[j]), 'wb') as pulsarMatchesDoc:                pickle.dump(matches, pulsarMatchesDoc)def getMatches(distances, max_seperation, min_seperation):    return (distances < max_seperation) & (distances > min_seperation)def loadPulsarData(stokesType, matchCategory, set_name):    with open(('{}_{}_{}_ASKAP_matches.table').format(stokesType, matchCategory, set_name),              'rb') as pulsarMatches:        return pickle.load(pulsarMatches)def createS888(pulsarData):    #hasS400 = pulsarData["S400"] != np.ma.core.MaskedConstant    #hasS1400 = pulsarData["S1400"] != np.ma.core.MaskedConstant    hasS400 = np.invert(np.isnan(pulsarData["S400"]))    hasS1400 = np.invert(np.isnan(pulsarData["S1400"]))    # Assigns values to S888 through linear scaling if either S400 or S1400 exist, otherwise logarithmic through polyfit    S888 = np.zeros(len(hasS400))    S888Error = np.zeros(len(hasS400))    for j in range(len(S888)):        if hasS400[j] and pulsarData["S400"][j] != 0 and hasS1400[j] and pulsarData["S1400"][j] != 0:            fluxes = [pulsarData["S400"][j], pulsarData["S1400"][j]]            flux_error = [pulsarData["S400_ERR"][j], pulsarData["S1400_ERR"][j]]            for k in range(len(flux_error)):                if type(flux_error[k]) != np.float64:                    flux_error[k] = 0.01            fit = np.polyfit(np.log([400, 1400]), np.log([pulsarData["S400"][j], pulsarData["S1400"][j]]), 1)            S888[j] = np.exp(np.polyval(fit, np.log(888)))            sigma = S888[j] * (flux_error[1] / fluxes[1] + flux_error[0] / fluxes[0]) * np.log(888) * 1 / np.log(                1400 / 400)            test_s888 = np.exp(fit[1]) * np.exp(np.log(fluxes[1] / fluxes[0]) / (np.log(1400 / 400)) * np.log(888))            S888Error[j] = sigma            if flux_error[0] != 0.01 and flux_error[1] != 0.01:                print(fluxes, flux_error, S888[j], sigma)                print(test_s888, S888[j])        elif hasS400[j] and pulsarData["S400"][j] != 0.0:            S888[j] = pulsarData["S400"][j] * (888.0 / 400) ** -1.5            S888Error[j] = S888[j] / 100        elif hasS1400[j] and pulsarData["S1400"][j] != 0.0:            S888[j] = pulsarData["S1400"][j] * (888.0 / 1400) ** -1.5            S888Error[j] = S888[j] / 100        else:            S888[j] = -1    print(len(S888))    return S888, S888Errordef compareRACSSources(catalog, catalogMatchFrom, racsType, max_separation, pulsarSetName):    # Checks distance between objects in stokesV and stokesI, returning both matches and failed matches    with open(catalogMatchFrom + '_' + racsType + "_" + pulsarSetName + '_ASKAP_matches.table', 'rb') as pulsarMatchesDoc:        matches = pickle.load(pulsarMatchesDoc)    racsi_data = Table.read(catalog + '_' + racsType + '.hdf5')    print("Comparing stokesI and stokesV coordinates")    racsV_coords = SkyCoord(matches[0]['ra_deg_cont'], matches[0]['dec_deg_cont'], unit=('deg', 'deg'))    racsI_coords = SkyCoord(racsi_data['ra_deg_cont'], racsi_data['dec_deg_cont'], unit=('deg', 'deg'))    imatch, d2, _ = racsV_coords.match_to_catalog_sky(racsI_coords)    print("Done matching stokesI and stokesV")    new_matches = {"racsi" :racsi_data[imatch[d2 < max_separation]], "racsv": matches[0][d2 < max_separation], "ASKAP": matches[1][d2 < max_separation],                   "distanceIandV": d2[d2 < max_separation], "distanceATNFandV": matches[2][d2 < max_separation]}    failed_matches = {"racsi": racsi_data[imatch[d2 > max_separation]], "racsv": matches[0][d2 > max_separation], "ASKAP": matches[1][d2 > max_separation],                      "distanceIandV": d2[d2 > max_separation], "distanceATNFandV": matches[2][d2 > max_separation]}    new_matches["racsi"].add_column(Column(racsI_coords[imatch[d2 < max_separation]], name="skycoord"))    failed_matches["racsi"].add_column(Column(racsI_coords[imatch[d2 > max_separation]], name="skycoord"))    with open('combined_matches' + "_" + racsType + '.table', 'wb') as pulsarMatchesDoc:        pickle.dump(new_matches, pulsarMatchesDoc)    with open('failed_matches' +"_" + racsType + '.table', 'wb') as pulsarMatchesDoc:        pickle.dump(failed_matches, pulsarMatchesDoc)    plt.clf()    for i in d2.arcsec:        print(i)    plt.hist(np.log10(d2.arcsec), bins=30, cumulative=True, log=True)    plt.xlabel('log10(Separation of pulsar in StokesI to StokesV) source [arcsec])')    plt.ylabel('Number of pulsars < separation')    plt.plot(np.log10(max_separation.to(u.arcsec).value) * np.array([1, 1]),            plt.gca().get_ylim(), 'r--')    plt.savefig(pulsarSetName + '_StokesIandV_separation.png')    return new_matches, failed_matchesdef fluxDensityGraph(matches_data, pulsarPath = "pulsars"):    # Creates a graphs of each pulsars spectral energy density as measured by ATNF, RACS, and our calculated values    hasS400 = np.invert(np.isnan(matches_data["ASKAP"]["S400"]))    hasS1400 = np.invert(np.isnan(matches_data["ASKAP"]["S1400"]))    plt.rcParams.update({"errorbar.capsize": 2})    for i in range(len(matches_data["ASKAP"])):        S400 = matches_data["ASKAP"]["S400"][i]        S1400 = matches_data["ASKAP"]["S1400"][i]        S400_ERR = matches_data["ASKAP"]["S400_ERR"][i]        S1400_ERR = matches_data["ASKAP"]["S1400_ERR"][i]        S888Calc = matches_data["ASKAP"]["S888Calc"][i]        S888Calc_ERR = matches_data["ASKAP"]["S888_ERR"][i]        S888i = matches_data["racsi"]["flux_int"][i]        S888i_ERR = matches_data["racsi"]["flux_int_err"][i]        S888v = matches_data["racsv"]["flux_int"][i]        S888v_ERR = matches_data["racsv"]["flux_int_err"][i]        fig = plt.figure()        ax = fig.add_subplot(111)        ax.set_ylabel(r'Spectral Energy Density $\times$ MHz [mJy]')        ax.set_xlabel('Frequency [MHz]')        ax.set_title("Log Graph of Flux Density vs. Frequency for: " + matches_data["ASKAP"]["NAME"][i])        if hasS400[i] and S400 != 0.0 and hasS1400[i] and S1400 != 0:            ax.loglog([400, 1400], [S400, S1400], "o", marker="D", markersize=10)            plt.errorbar(400, S400, yerr=S400_ERR, color="black")            plt.errorbar(1400, S1400, yerr=S1400_ERR, color="black")        elif hasS400[i] and S400 != 0.0:            ax.loglog([400], [S400], "o", marker="D", markersize=10)            plt.errorbar(400, S400, yerr=S400_ERR, color="black")        elif hasS1400[i] and S1400 != 0.0:            ax.loglog([1400], [S1400], "o", marker="D", markersize=10)            plt.errorbar(1400, S1400, yerr=S1400_ERR, color="black")        ax.loglog([888], [S888Calc], "o", marker="o", markersize=10)        plt.errorbar(888, S888Calc, yerr=S888Calc_ERR, color="black")        ax.loglog([888], [S888v], "o", marker="s", markersize=10)        plt.errorbar(888, S888v, yerr=S888v_ERR, color="black")        ax.loglog([888], [S888i], "o", marker="*", markersize=15)        plt.errorbar(888, S888i, yerr=S888i_ERR, color="black")        if hasS400[i] and hasS1400[i]:            try:                x = np.array([400, 888, 1400])                scaleValue = scaleFrequency(x, "mean")                x = np.multiply(x, scaleValue)                y = np.array([S400, S888i, S1400])                yErr = np.array([S400_ERR, S888i_ERR, S1400_ERR])                spectral_index, cov = np.polyfit(np.log10(x), np.log10(y), 1, w=y/yErr, cov="unscaled")                #spectral_index, cov = curve_fit(slopeFunc, np.log10(x), np.log10(y), sigma=yErr/y)                sigma = np.sqrt(np.diag(cov))[0]                lineXVals = np.array([350, 888, 1450]) * scaleValue                lineYVals = list((j ** spectral_index[0]) * np.power(10, spectral_index[1]) for j in lineXVals)                plt.loglog(lineXVals * 1/scaleValue, lineYVals, "--", color="black")                lineYWithHighErrors = list((j ** (spectral_index[0] + sigma)) * np.power(10, spectral_index[1]) for j in lineXVals)                lineYWithLowErrors = list((j ** (spectral_index[0] - sigma)) * np.power(10, spectral_index[1]) for j in lineXVals)                plt.fill_between(lineXVals * 1/scaleValue, y1=lineYWithHighErrors, y2=lineYVals, color="0.5")                plt.fill_between(lineXVals * 1/scaleValue, y1=lineYWithLowErrors, y2=lineYVals, color="0.5")                text = r'$\alpha = {} \pm {}$'.format(str(round(np.around(spectral_index[0], 3), 2)), np.around(sigma, 2))                plt.text(0.01, 0.30, text, transform=ax.transAxes)                makeFDvsFInfo(scaleValue, x, y, yErr, lineYWithHighErrors, lineYWithLowErrors, sigma, spectral_index, cov, pulsarPath + "/" + matches_data["ASKAP"]["NAME"][i])            except Exception as e:                print(e)        ax.legend(["ATNF", "Calculated", "RACSV", "RACSI"], loc=3)        try:            plt.savefig(pulsarPath + "/" + matches_data["ASKAP"]["NAME"][i] + "/FDvsF" + ".png")        except:            os.mkdir(pulsarPath + "/" + matches_data["ASKAP"]["NAME"][i])            plt.savefig(pulsarPath + "/" + matches_data["ASKAP"]["NAME"][i] + "/FDvsF"  + ".png")        print("Made Spectral energy density figure: " + str(i) + "/" + str(len(matches_data["racsv"])))        plt.close()def scaleFrequency(x, scaleType=False):    # Scales the frequencies used in the spectral indecies of the flux density graph    if scaleType == "mean":        return 1 / np.mean(x)    else:        return 1 / x[1]    returndef makeFDvsFInfo(scaleValue, x, y, yErr, lineYWithHighErrors, lineYwithLowErrors, sigma, spectral_index, cov, filePath):    # This creates info text on the flux density vs frequency graphs and saves it in the pulsar's folder as a text document    if not os.path.isdir(filePath):        os.mkdir(filePath)    info = ('scaleValue: {} \n' \           'x: {} \n' \           'y: {} \n' \           'y Error: {} \n' \           'y with positive sigma additional slope: {} \n' \           'y with negative sigma additional slope: {} \n' \           'Sigma: {} \n' \           'Spectral Index: {} \n' \           'Covariance Matrix: {}').format(scaleValue, x, y, yErr, lineYWithHighErrors, lineYwithLowErrors, sigma, spectral_index, cov)    try:        file = open(filePath + "/FDvsF_Info", "x")    except:        file = open(filePath + "/FDvsF_Info", "w")    file.write(info)def makeCutouts(mosaicFilePaths, pulsarData, racsType):    # uses the cutout function to zoom in on the pulsars in the RACS catalog mosaics    # Generates the names of mosaics that each pulsar is located within, and fetches the coordinates    sizes = [0.03, 0.1, 0.3, 0.9]    stokesVersions = ["i", "v"]    pos_change = -3 if racsType == "islands" else 0    for i in stokesVersions:        if i == "v":            file_names = [mosaicFilePaths[0] + j.strip()[(len(j) - (46 + pos_change)):(len(j) - (22 + pos_change))] + ".fits" for j in pulsarData["racs" + i]["fileName"]]        else:            file_names = [mosaicFilePaths[1] + j.strip()[(len(j) - (48 + pos_change)):(len(j) - (24 + pos_change))] + ".fits" for j in pulsarData["racs" + i]["fileName"]]    # Takes coordinates and file names and creates cutouts of multiple sizes.    # center of the image is ATNF, as is the blue lines.        for j in range(len(file_names)):            print("Making cutouts of " + pulsarData["ASKAP"]["NAME"][j]+ " in stokes" + i + ": " + str(j) + "/" + str(len(file_names)))            for k in sizes:                try:                    if racsType == "both":                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                      [pulsarData["racsv"][j], pulsarData["racsvIslands"][j]],                                      [pulsarData["racsi"][j],pulsarData["racsiIslands"][j]], i, racsType)                    else:                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                pulsarData["racsv"][j], pulsarData["racsi"][j], i, racsType)                except:                    os.mkdir("pulsars/" + pulsarData["ASKAP"]["NAME"][j])                    if racsType == "both":                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                      [pulsarData["racsv"][j], pulsarData["racsvIslands"][j]],                                      [pulsarData["racsi"][j],pulsarData["racsiIslands"][j]], i, racsType)                    else:                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                pulsarData["racsv"][j], pulsarData["racsi"][j], i, racsType)def makeCutouts2(mosaicFilePaths, pulsarData, pulsarFilePath, dataTypes=["island", "components", "both"]):    # This takes a combined pulsar data to create and save a variety of cutouts    # Cutouts are zoomed in stokes mosaics on the coordinates of different pulsars with the different positions in catalogs marked    mosaicTypes = ["stokesV", "stokesI"]    sizes = [0.03, 0.1, 0.3, 0.9]    for i in range(len(pulsarData["ASKAP"])):        for j in mosaicTypes:            #print(pulsarData["racsi"]["fileName"])            #pos_change = 5 if pulsarData["racsv"]["stokesType"][i] == "components" else 0            if j == "stokesV":                file_name = mosaicFilePaths[0] + "RACS_" +  pulsarData["racsi"]["fileName"][i] + ".EPOCH00.V.fits"            else:                file_name = mosaicFilePaths[1] + "RACS_" +  pulsarData["racsi"]["fileName"][i] + ".EPOCH00.I.fits"            for k in dataTypes:                markers = [pulsarData["ASKAP"][i]]                if k == "island" or k == "both":                    markers.append(pulsarData["racsiIslands"][i])                    markers.append(pulsarData["racsvIslands"][i])                elif k =="components" or k == "both":                    markers.append(pulsarData["racsi"][i])                    markers.append(pulsarData["racsv"][i])                for x in sizes:                    try:                        cutout.create_cutout(file_name, "Flux Density Vs. Position for: " +                                         pulsarData["ASKAP"]["NAME"][i] + " (" + j +")", x, markers, j, pulsarData["ASKAP"]["skycoord"][i], "total", k, pulsarFilePath=pulsarFilePath)                    except Exception as e:                        print(e)        print("Created cutout for pulsar {} of {}".format(i + 1, len(pulsarData["ASKAP"])))def makePulsarInfo(pulsarData, pulsarFilePath):    # This creates a text document of different information relating to a single pulsar and saves it in the pulsars folder    for i in range(len(pulsarData["ASKAP"])):        atnf = pulsarData["ASKAP"][i]        racsi = pulsarData["racsi"][i]        racsv = pulsarData["racsv"][i]        distanceATNFandV = 0 #pulsarData["distanceATNFandV"][i].quantity        distanceIandV = 0 #pulsarData["distanceIandV"][i].quantity        # fix later        try:            file = open(pulsarFilePath + "/" + atnf["NAME"] + "/info", "x")        except Exception as e:            file = open(pulsarFilePath + "/" + atnf["NAME"] + "/info", "w")        S400_ERR = str(round(float(atnf["S400_ERR"]), 2))\            if not math.isnan(atnf["S400_ERR"]) else "N/A"        S1400_ERR = str(round(float(atnf["S1400_ERR"]), 2))\            if not math.isnan(atnf["S1400_ERR"]) else "N/A"        S888 = str(round(float(atnf["S888Calc"]), 2)) \            if not math.isnan(atnf["S888Calc"]) else "N/A"        S888_ERR = str(round(float(atnf["S888_ERR"]), 2)) \            if not math.isnan(atnf["S888_ERR"]) else "N/A"        fileInfo = """Position     ATNF: ({} +- {} [Degrees], {} +- {} [Degrees])    StokesV: ({} +- {} [Degrees], {} +- {} [Degrees])    StokesI: ({} +- {} [Degrees], {} +- {} [Degrees])Distance     ATNF StokesV: {} [Degrees]    StokesV StokesI: {} [Degrees]Flux Density    S400: {} +- {} [mJy]    S888Calc: {} +- {} [mJy]    S888I: {} +- {} [mJy]    S888V: {} +- {} [mJy]    S1400: {} +- {} [mJy]    Polarized Fraction: {} [mJy]    SD V: {}    SD IQR V: {}    SD I: {}    SD IQR I: {}        """.format(round(float(atnf["RAJD"]), 4), round(float(atnf["RAJD_ERR"]), 6),                   round(float(atnf["DECJD"]), 4), round(float(atnf["DECJD_ERR"]), 6),                   round(float(racsv["ra_deg_cont"]), 4), round(float(racsv["ra_err"]) * (1/3600), 6),                   round(float(racsv["dec_deg_cont"]), 4), round(float(racsv["dec_err"]) * (1/3600), 6),                   round(float(racsi["ra_deg_cont"]), 4), round(float(racsi["ra_err"]) * (1/3600), 6),                   round(float(racsi["dec_deg_cont"]), 4), round(float(racsi["dec_err"]) * (1/3600), 6),                   round(float(distanceATNFandV) * (1/3600), 4),                   round(float(distanceIandV) * (1/3600), 4),                   str(round(float(atnf["S400"]), 2)), S400_ERR,                   S888, S888_ERR,                   str(round(float(racsi["flux_int"]), 2)), str(round(float(racsi["flux_int_err"]), 2)),                   str(round(float(racsv["flux_int"]), 2)), str(round(float(racsv["flux_int_err"]), 2)),                   str(round(float(atnf["S1400"]), 2)), S1400_ERR,                   str(round(float(racsv["flux_int"]) / (float(racsi["flux_int"])), 5)),                   str(round(racsv["sigmaOfMosaic"],9)), str(round(racsv["sigmaOfMosaicIQR"],9)),                   str(round(racsi["sigmaOfMosaic"], 9)), str(round(racsi["sigmaOfMosaicIQR"], 9))                   )        file.write(fileInfo)def combineIslandsComponents(pulsarDataIslands, pulsarDataComponents):    # This returns a dictionary of tables containing information from both islands and components    matches_islands = []    matches_components = []    for i in range(len(pulsarDataIslands["ASKAP"])):        for j in range(len(pulsarDataComponents["ASKAP"])):            if pulsarDataIslands["ASKAP"]["NAME"][i] == pulsarDataComponents["ASKAP"]["NAME"][j]:                matches_islands.append(i)                matches_components.append(j)    new_pulsar_data = {"ASKAP":pulsarDataIslands["ASKAP"][matches_islands], "racsiIslands":pulsarDataIslands["racsi"][matches_islands],                       "racsi":pulsarDataComponents["racsi"][matches_components], "racsvIslands":pulsarDataIslands["racsv"][matches_islands],                       "racsv":pulsarDataComponents["racsv"][matches_components]}    return new_pulsar_datadef compareIslandsComponentsCutout(mosaicFilePaths, tablePaths, fileNameV, fileNameI):    # This creates multiple cutout of entire Stokes regions to compare Components and Islands    racsV_data = vstack(racsTableCompute(tablePaths[0],"RACS_StokesV", fileNameV))    racsI_data = vstack(racsTableCompute(tablePaths[1],"RACS_StokesI", fileNameI))    combined_data = vstack([racsV_data, racsI_data])    pos_change = -3 if racsV_data["type"][0] == "islands" else 0    file_name = racsV_data["fileName"][0].strip()[(len(racsV_data["fileName"][0])                - (43 + pos_change)):(len(racsV_data["fileName"][0]) - (19 + pos_change))] + ".fits"    cutout.create_cutout(mosaicFilePaths[0] + file_name, "StokesV Islands & Components Comparison: 0000-18A",                         25, racsV_data, "StokesV", legend="first")    cutout.create_cutout(mosaicFilePaths[1] + file_name, "StokesI Islands & Components Comparison: 0000-18A",                         25, racsI_data, "StokesI", legend="first")    cutout.create_cutout(mosaicFilePaths[1] + file_name,                         "StokesI/V Islands & Components Comparison Base StokesV: 0000-18A",                         25, combined_data, "StokesICombined", legend="first")    cutout.create_cutout(mosaicFilePaths[0] + file_name,                         "StokesI/V Islands & Components Comparison Base StokesV: 0000-18A",                         25, combined_data, "StokesVCombined", legend="first")def makeRegionFile(objects, regionName, color, markerType="circle"):    # Makes as region file for DS9 using a table of pulsars    region_info = "# Region file format: DS9 version 4.1\nglobal color=" + color + ' dashlist=8 3 width=1 font="helvetica 10 normal roman" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\nfk5'    for i in objects:        if markerType == "circle":            region_info += "\n" + "circle(" + str(i["ra_hms_cont"]) + "," + str(i["dec_dms_cont"]) + "," + str(60) + '")'        elif markerType == "box":            region_info += "\n" + "box(" + str(i["ra_hms_cont"]) + "," + str(i["dec_dms_cont"]) + "," + str(60) + '",' + str(60) + '")'    file = open(regionName, "w")    file.write(region_info)    file.close()def createImages(pulsarData, pulsarFilePath):    # Creates graphics using pil of the different cutouts, graphs, and info documents created in the rest of the code    imageSize = (600, 400)    for i in pulsarData["ASKAP"]["NAME"]:        try:            sizes = [0.03, 0.1, 0.3, 0.9]            cutOutConI = list(Image.open(pulsarFilePath + "/{}/cutout{},stokesI,components.png".format(i, j)).resize(imageSize) for j in sizes)            cutOutConV = list(Image.open(pulsarFilePath + "/{}/cutout{},stokesV,components.png".format(i, j)).resize(imageSize) for j in sizes)            #cutOutIonI = list(Image.open("pulsars/{}/cutout{}i_islands.png".format(i, j)) for j in sizes)            #cutOutIonV = list(Image.open("pulsars/{}/cutout{}v_islands.png".format(i, j)) for j in sizes)            infoImage = makeReadablePic(pulsarFilePath + "/{}/info".format(i), imageSize)            try:                infoFDImage = makeReadablePic(pulsarFilePath +"/{}/FDvsF_Info".format(i), imageSize)            except:                infoFDImage = Image.new('RGB', imageSize, (255, 255, 255, 0))            fluxDensityGraph = Image.open(pulsarFilePath + "/{}/FDvsF.png".format(i)).resize(imageSize)            blank = Image.new('RGB', imageSize, (255, 255, 255, 0))            image1 = cutout.createImage([cutOutConI[2], cutOutConV[2], fluxDensityGraph, infoImage], [2,2], imageSize)            cutOutPics = []            for j in range(len(sizes)):                cutOutPics.append(cutOutConI[j])                cutOutPics.append(cutOutConV[j])            image2 = cutout.createImage(cutOutPics, [2,4], imageSize)            image3 = cutout.createImage([fluxDensityGraph, infoImage, infoFDImage, blank], [2,2], imageSize)            try:                image1.save(pulsarFilePath + "/" + i + "/combined/main.png")                image2.save(pulsarFilePath + "/"  + i + "/combined/cutouts.png")                image3.save(pulsarFilePath + "/"  + i + "/combined/FDinfo.png")            except:                os.mkdir(pulsarFilePath + "/"  + i + "/combined")                image1.save(pulsarFilePath + "/"  + i + "/combined/main.png")                image2.save(pulsarFilePath + "/"  + i + "/combined/cutouts.png")                image3.save(pulsarFilePath + "/"  + i + "/combined/FDinfo.png")            print("Graphic created for pulsar: {}".format(i))        except Exception as e:            print(e)def makeGoodPulsarTable(pulsars, ratings):    RACSI_match = np.array(np.zeros(len(pulsars["ASKAP"])), dtype=bool)    RACSV_match = np.array(np.zeros(len(pulsars["ASKAP"])), dtype=bool)    to_keep = np.array(np.zeros(len(pulsars["ASKAP"])), dtype=bool)    for i in range(len(pulsars["ASKAP"])):        if pulsars["ASKAP"]["NAME"][i] in ratings:            RACSV_match[i] = True if ratings[pulsars["ASKAP"]["NAME"][i]]["racsVMatch"] == True else False            RACSI_match[i] = True if ratings[pulsars["ASKAP"]["NAME"][i]]["racsIMatch"] == True else False            if (RACSI_match[i] or RACSV_match[i]) and ratings[pulsars["ASKAP"]["NAME"][i]]["needs_recompute"]:                if pulsars["ASKAP"]["NAME"][i] == "J1948+3540": # The only match that is RACSV only                    RACSV_match[i] == True                    RACSI_match[i] == False                else:                    RACSV_match[i] == False                    RACSI_match[i] == True            to_keep[i] = True        else:            RACSV_match[i] = False            RACSI_match[i] = False            to_keep[i] = False    for i in range(len(RACSV_match)):        if not RACSV_match[i]:            pulsars["racsv"]["flux_peak"][i] = 0            pulsars["racsv"]["flux_int"][i] = 0            pulsars["racsv"]["flux_int_err"][i] = 0            pulsars["racsv"]["ra_deg_cont"][i] = 0            pulsars["racsv"]["dec_deg_cont"][i] = 0            pulsars["racsv"]["ra_err"][i] = 0            pulsars["racsv"]["dec_err"][i] = 0            pulsars["distanceIandV"][i] = 0            pulsars["distanceATNFandV"][i] = 0    for i in range(len(RACSI_match)):        if not RACSI_match[i]:            pulsars["racsi"]["flux_peak"][i] = 0            pulsars["racsi"]["flux_int"][i] = 0            pulsars["racsi"]["flux_int_err"][i] = 0            pulsars["racsi"]["ra_deg_cont"][i] = 0            pulsars["racsi"]["dec_deg_cont"][i] = 0            pulsars["racsi"]["ra_err"][i] = 0            pulsars["racsi"]["dec_err"][i] = 0            pulsars["distanceIandV"][i] = 0    pulsars["racsv_match"] = RACSV_match    pulsars["racsi_match"] = RACSI_match    for i in pulsars.keys():        pulsars[i] = pulsars[i][to_keep]    return pulsarsdef makeReadablePic(fileName, imageSize):    #Takes a file name and image size to create an image with the text from the file on it    with open(fileName, "rb") as info:        infoImage = Image.new('RGB', (int(imageSize[0] / 1.4), int(imageSize[1] / 1.4)), (255, 255, 255, 0))        draw = ImageDraw.Draw(infoImage)        textLines = str(info.read()).split("\\n")        text = []        for j in textLines:            for k in textwrap.wrap(j, width=65):                text.append(k)        text = "\n".join(text)        text = text[2:]        text = text[:-1]        draw.text((0, 0), text, fill="black")        infoImage = infoImage.resize(imageSize)        return infoImagedef createCombinedPulsarTable(newPulsarData, pulsarTable):    if pulsarTable == None:        pulsarTable = {}        for i in newPulsarData.keys():            if astropy.table.table.Table == type(newPulsarData[i]):                pulsarTable[i] = QTable(list([] for i in range(len(newPulsarData[i].columns.keys()))),                             names=newPulsarData[i].columns.keys())            else:                pulsarTable[i] = np.array([])    for i in pulsarTable.keys():        try:            pulsarTable[i] = vstack([pulsarTable[i], newPulsarData[i]])        except Exception as e:            print(e)    return pulsarTabledef savePulsarTable(pulsarData):    columnsSaveInfo = {"ASKAP":[["NAME","NAME"],["S400","S400_PSRCAT"], ["S1400","S1400_PSRCAT"]            ,["S400_ERR","S400_PSRCAT_ERR"], ["S1400_ERR","S1400_PSRCAT_ERR"], ["RAJD","RAJD_PSRCAT"],            ["DECJD","DECJD_PSRCAT"], ["RAJD_ERR","RAJD_PSRCAT_ERR"], ["DECJD_ERR","DECJD_PSRCAT_ERR"],            ["S888Calc","S888_PSRCAT"], ["S888_ERR","S888_PSRCAT_ERR"]],        "racsv":[["flux_peak","S888_RACSV"],["flux_int","S888_int_RACSV"], ["flux_int_err","S888_int_RACSV_ERR"],            ["sigmaOfMosaicIQR","detectionSigma_RACSV"],            ["hasUpperLimit","isUpperLimit_RACSV"],            ["ra_deg_cont","RAJD_RACSV"], ["dec_deg_cont","DECJD_RACSV"],            ["ra_err","RAJD_RACSV_ERR"], ["dec_err","DECJD_RACSV_ERR"]],        "racsi":[["flux_peak","S888_RACSI"],["flux_int","S888_int_RACSI"], ["flux_int_err","S888_int_RACSI_ERR"],                 ["sigmaOfMosaicIQR", "detectionSigma_RACSI"],                 ["hasUpperLimit", "isUpperLimit_RACSI"],                 ["ra_deg_cont","RAJD_RACSI"], ["dec_deg_cont","DECJD_RACSI"], ["ra_err","RAJD_RACSI_ERR"],                 ["dec_err","DECJD_RACSI_ERR"]]}    columns = []    keys = columnsSaveInfo.keys()    nameList = []    for i in keys:        for j in columnsSaveInfo[i]:            if j[0] == "dec_err" or j[0] == "ra_err":                #columns.append(pulsarData[i][j[0]].to(u.deg))                columns.append(pulsarData[i][j[0]].to(u.deg))            else:                columns.append(pulsarData[i][j[0]])            nameList.append(j[1])    for i in ["RACSV","RACSI"]:        nameList.append("Distance_PSRCAT_" + i)        #print(pulsarData["ASKAP"]["RAJD"].to(u.rad),pulsarData["ASKAP"]["DECJD"].to(u.rad) )        #askapPositions = SkyCoord(ra=pulsarData["ASKAP"]["RAJD"].quantity, dec=pulsarData["ASKAP"]["DECJD"].quantity,  unit=('deg', 'deg'))        askapPositions = astropy.coordinates.concatenate(pulsarData["ASKAP"]["skycoord"])        stokes = SkyCoord(ra=pulsarData[i.lower()]["ra_deg_cont"], dec=pulsarData[i.lower()]["dec_deg_cont"], unit=('deg', 'deg'))        sep = askapPositions.separation(stokes)        sep = sep.deg * u.deg        for j in range(len(pulsarData[i.lower()]["hasUpperLimit"])):            if pulsarData[i.lower()]["hasUpperLimit"][j]:                sep[j] = 0        columns.append(sep)    combinedTable = QTable(columns, names=nameList)    combinedTable.write("combinedPulsars.hdf5", overwrite=True, serialize_meta = True)if len(sys.argv) != 1 and sys.argv[1] == "mortimer":    path_stokes_I_mos = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESI_IMAGES/"    path_stokes_V_mos = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESV_IMAGES/"    path_stokes_I_tables = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESI_SELAVY"    path_stokes_V_tables = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESV_SELAVY"    pathStokesIComponents = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESI_SELAVY/VAST_Epoch0_StokesI_components.hdf5"    pathStokesVComponents = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESV_SELAVY/VAST_Epoch0_StokesV_components.hdf5"else:    path_stokes_I_mos = ""    path_stokes_V_mos = "mos/"    path_stokes_I_tables = "tables/racs_cati"    path_stokes_V_tables = "tables/racs_catv"if len(sys.argv) > 2 and sys.argv[2] == "redownload":    # Here it re-downloads all the data    #racsTableCompute(path_stokes_V_tables, "RACS_StokesV")    #racsTableCompute(path_stokes_I_tables, "RACS_StokesI")    names = np.array([""])    max_separation = [25 * u.arcsec]    min_separation = [0 * u.arcsec]    separations = QTable([names, max_separation, min_separation], names=["name", "max_separation", "min_separation"])    getASKAP(separations, pathStokesVComponents)print("here1")if len(sys.argv) > 1 and sys.argv[1] == "mortimer":    pulsar_folder = "pulsars/"    for i in [""]:        print("here2")        pulsar_table_total = None        set_names = ["both_match"]        #set_names = ["both_match", "only_racsV", "only_racsI"]        for j in set_names:            try:                os.mkdir("{}{}_{}_pulsars".format(pulsar_folder, i, j))            except:                pass            print("here4")            pulsar_data = loadPulsarData("components", i, j)            with open('final_pulsars.dict', 'rb') as ratingsFile:                ratings = pickle.load(ratingsFile)            #pulsar_data = makeGoodPulsarTable(pulsar_data, ratings)            print("here5")            #fluxDensityGraph(pulsar_data, pulsarPath="{}{}_{}_pulsars".format(pulsar_folder, i, j))            makeCutouts2([path_stokes_V_mos, path_stokes_I_mos], pulsar_data, "{}{}_{}_pulsars".format(pulsar_folder, i, j), ["components"])            #pulsar_data = cutout.get_rms(pulsar_data, [path_stokes_V_mos, path_stokes_I_mos], "{}{}_{}_pulsars".format(pulsar_folder, i, j), "racsv", 20, creating_upper_limits=False)            #pulsar_data = cutout.get_rms(pulsar_data, [path_stokes_V_mos, path_stokes_I_mos], "{}{}_{}_pulsars".format(pulsar_folder, i, j), "racsi", 20, creating_upper_limits=False)            #makePulsarInfo(pulsar_data, "{}{}_{}_pulsars".format(pulsar_folder, i, j))            #createImages(pulsar_data, "{}{}_{}_pulsars".format(pulsar_folder, i, j))            #pulsar_table_total = createCombinedPulsarTable(pulsar_table_total, pulsar_data)            for k in pulsar_data["ASKAP"]:                if "J1748-2446" in k["NAME"]:                    print(k["ASSOC"])if len(sys.argv) == 1: # For test runs on my laptop    racsTableCompute(path_stokes_V_tables, "RACS_StokesV")    getASKAP("RACS_StokesV")