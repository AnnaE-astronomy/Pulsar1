import sysfrom astropy.table import Table, vstack, Columnimport pandasimport globimport numpy as npimport psrqpy as psfrom astropy.coordinates import SkyCoordfrom astropy import units as u, constants as cimport pickleimport matplotlib.pyplot as pltimport mathimport osimport cutoutfrom os.path import expanduserfrom astropy.coordinates import Anglefrom scipy.optimize import curve_fitimport scipyfrom PIL import Image, ImageDraw, ImageFontimport textwrapfileInfoText = """Position     ATNF: ({} +- {} [Degrees], {} +- {} [Degrees])    StokesV: ({} +- {} [Degrees], {} +- {} [Degrees])    StokesI: ({} +- {} [Degrees], {} +- {} [Degrees])Distance     ATNF StokesV: {} [Degrees]    StokesV StokesI: {} [Degrees]Flux Density    S400: {} +- {} [mJy]    S888Calc: {} +- {} [mJy]    S888I: {} +- {} [mJy]    S888V: {} +- {} [mJy]    S1400: {} +- {} [mJy]    Polarized Fraction: {} [mJy]        """def slopeFunc(x, a, b):    return a * x + bdef racsTableCompute(path_tables, catalog, specific_table=""):    # time left this week, 2 hour    # Goes through all the components in the racs_catv tables which hold the detected objects in the mosaics    # and creates a table out of them all    racs_tables = []    for type in ['islands', 'components']:        if specific_table == "":            selavy_files = sorted(glob.glob((path_tables + '/*{0}.txt').format(type)))        else:            selavy_files = [(path_tables + '/' + specific_table + '{0}.txt').format(type)]        data = []        for i, file in enumerate(selavy_files):            table = Table.from_pandas(pandas.read_fwf(file, skiprows=[1, ]))            table.add_column(Column(np.array(file), name="fileName"))            table.add_column(Column(np.array(type), name="stokesType"))            table.add_column(Column(np.array(catalog), name="type"))            data.append(table)            print('Read {0} lines from {1} ({2}/{3}) \r'.format(len(data[-1]),                                                                file, i, len(selavy_files)))        racs_data = vstack(data)        # these are empty and confuse HDF5        del racs_data['#']        del racs_data['comment']        if specific_table == "":            racs_data.write((catalog + '_{0}.hdf5').format(type), overwrite=True)            print(('Wrote to ' + catalog + '_{0}.hdf5').format(type))        else:            racs_tables.append(racs_data)    return racs_tablesdef getASKAP(catalog, racsType):    # from professor Kaplan    max_position_uncertainty = 1 * u.arcsec    max_separation = 5 * u.arcsec    columns = ['Name',               'RAJ',               'DECJ',               'RAJD',               'DECJD',               'DM',               'S400',               'S1400',               'S2000',               'ASSOC',               'DIST',               # 'L400',               # 'L1400',               ]    requery = True    if requery:        query = ps.QueryATNF(params=columns)        data = query.table        with open('pulsars.table', 'wb') as pulsarDoc:            pickle.dump(data, pulsarDoc)    else:        with open('pulsars.table', 'rb') as pulsarDoc:            data = pickle.load(pulsarDoc)    # it's faster to make a coordinate object out of floats than strings    coords = SkyCoord(data['RAJD'], data['DECJD'], unit=('deg', 'deg'))    data.add_column(Column(coords, name="skycoord"))    data.add_column(Column(np.array("ATNF"), name="type"))    # let's do some selections on the coordinates and catalog parameters    # we need the cos(Dec) term in here because an error in RA (or any longitude)    # changes near the poles    position_uncertainty = np.sqrt(        (data['RAJD_ERR'].quantity * np.cos(data['DECJD'].quantity)) ** 2 + (data['DECJD_ERR'].quantity) ** 2)    # it's probably not a bad idea to make a histogram of these    plt.clf()    plt.hist(np.log10(position_uncertainty.to(u.arcsec).value), bins=30, cumulative=True)    plt.xlabel('log10(Position uncertainty [arcsec])')    plt.ylabel('Number of pulsars < uncertainty')    plt.plot(np.log10(max_position_uncertainty.to(u.arcsec).value) * np.array([1, 1]),             plt.gca().get_ylim(), 'r--')    plt.title("Log Histrogram of the distance between the closest objects in StokesI and StokesV")    plt.savefig('position_uncertainty.png')    # make sure it's not in a globular cluster    globular_cluster = np.array(['GC' in x for x in data['ASSOC']])    # by using astropy units,  we can be assured that the position_uncertainty (in deg) and the    # max_position_uncertainty (in arcsec) are handled correctly    good = (position_uncertainty < max_position_uncertainty) & (~np.isnan(position_uncertainty)) & (~globular_cluster)    print('Total number of pulsars: %d' % len(data))    print('Number not in globular cluster and with position uncertainty < %.1f arcsec: %d' % (        max_position_uncertainty.to(u.arcsec).value,        good.sum()))    # now select the ones we like    data = data[good]    coords = coords[good]    position_uncertainty = position_uncertainty[good]    # read in RACS    racs_data = Table.read(catalog + '_' + racsType + '.hdf5')    racs_coords = SkyCoord(racs_data['ra_deg_cont'], racs_data['dec_deg_cont'], unit=('deg', 'deg'))    racs_data.add_column(Column(np.array(racs_coords), name="skycoord"))    imatch, d2, _ = coords.match_to_catalog_sky(racs_coords)    # it's probably not a bad idea to make a histogram of the separations    # aa cumulative histogram will enable us to judge how many are on each side of the line    # and we will plot log(separation)    # since we want to see a small number total, use log=True for the y-axis too    plt.clf()    plt.hist(np.log10(d2.arcsec), bins=30, cumulative=True, log=True)    plt.xlabel('log10(Separation from pulsar to ' + catalog + ' source [arcsec])')    plt.ylabel('Number of pulsars < separation')    plt.plot(np.log10(max_separation.to(u.arcsec).value) * np.array([1, 1]),             plt.gca().get_ylim(), 'r--')    plt.savefig(catalog + '_separation.png')    print('Number with separation < %.1f arcsec: %d' % (        max_separation.to(u.arcsec).value, (d2 < max_separation).sum()))    for i in range(len(data)):        if d2[i] < max_separation:            print('PSR %s matches component %s: flux_peak=%.1f mJy, separation=%.1f arcsec' % (                data[i]['NAME'],                racs_data[imatch[i]][racsType[:-1] + '_name'],                racs_data[imatch[i]]['flux_peak'],                d2[i].arcsec))    # Creates a list of both the table of matches and table of pulsars which correspond to those matches    matches_data = [racs_data[imatch[d2 < max_separation]], data[d2 < max_separation], d2[d2 < max_separation]]    #matches_data[1]["S400"] = np.array([(i if i !=0 else np.ma.core.MaskedConstant) for i in matches_data[1]["S400"]])    #matches_data[1]["S1400"] = np.array([(i if i !=0 else np.ma.core.MaskedConstant) for i in matches_data[1]["S1400"]])    hasS400 = matches_data[1]["S400"] != np.ma.core.MaskedConstant    hasS1400 = matches_data[1]["S1400"] != np.ma.core.MaskedConstant    # Assigns values to S888 through linear scaling if either S400 or S1400 exist, otherwise logarithmic through polyfit    S888 = np.zeros(len(hasS400))    S888Error = np.zeros(len(hasS400))    amount_no_errS400 = 0    amount_no_errS1400 = 0    relativeUncertenties = {"S400":[], "S1400":[]}    for i in range(len(S888)):        if hasS400[i] and matches_data[1]["S400"][i] != 0 and hasS1400[i] and matches_data[1]["S1400"][i] != 0:            fluxes = [matches_data[1]["S400"][i], matches_data[1]["S400"][i]]            flux_error = [matches_data[1]["S400_ERR"][i], matches_data[1]["S400_ERR"][i]]            for j in range(len(flux_error)):                if type(flux_error[j]) != np.float64:                    flux_error[j] = 0.01                                fluxes = np.log(np.array(fluxes))            flux_error = np.log(np.array(flux_error))            fit = np.polyfit(np.log([400, 1400]), np.log([matches_data[1]["S400"][i], matches_data[1]["S1400"][i]]), 1)            S888[i] = np.exp(np.polyval(fit, np.log(888)))            #spectral_index, cov = curve_fit(slopeFunc, np.log10([400, 1400]), fluxes, sigma=flux_error / fluxes)            #sigma = math.sqrt(np.diag(cov)[0] * np.diag(cov)[1])            #print("FIT: ", spectral_index, cov, sigma             #     , matches_data[1]["NAME"][i])            #S888[i] = np.exp(np.polyval(spectral_index, np.log(888)))            #S888Error[i] = sigma            S888Error[i] = S888[i] / 100        elif hasS400[i] and matches_data[1]["S400"][i] != 0.0:            S888[i] = matches_data[1]["S400"][i] * (888.0 / 400) ** -1.5            S888Error[i] = S888[i] / 100            if type(matches_data[1]["S400_ERR"][i]) != np.float64:                amount_no_errS1400 += 1            else:                pass                #relativeUncertenties["S400"].append(matches_data[1]["S400_ERR"][i] / matches_data[1]["S400"][i])        elif hasS1400[i] and matches_data[1]["S1400"][i] != 0.0:            S888[i] = matches_data[1]["S1400"][i] * (888.0 / 1400) ** -1.5            S888Error[i] = S888[i] / 100            if type(matches_data[1]["S1400_ERR"][i]) != np.float64:                amount_no_errS1400 += 1            else:                #relativeUncertenties["S1400"].append(matches_data[1]["S1400_ERR"][i] / matches_data[1]["S1400"][i])                pass        else:            S888[i] = -1    #print("AMOUNT s400:::", amount_no_errS400, "total: ", len(relativeUncertenties["S400"]) + amount_no_errS400)    #print("AMOUNT s1400:::", amount_no_errS1400, "total: ", len(relativeUncertenties["S1400"]) + amount_no_errS1400)    #print("S400 relative uncertenty average: ", sum(relativeUncertenties["S400"]) / len(relativeUncertenties["S400"]))    #print("S400 std", np.std(relativeUncertenties["S400"]))    #print("S1400 relative uncertenty average: ", sum(relativeUncertenties["S1400"]) / len(relativeUncertenties["S1400"]))    #print("S1400 std", np.std(relativeUncertenties["S1400"]))    print(len(S888))    matches_data[1].add_column(Column(S888, name="S888Calc"))    matches_data[1].add_column(Column(S888Error, name="S888_ERR"))    with open(catalog + '_'+ racsType + '_ASKAP_matches.table', 'wb') as pulsarMatchesDoc:        pickle.dump(matches_data, pulsarMatchesDoc)def compareRACSPulsars(catalog, pulsarMatches, racsType):    # Checks distance between objects in stokesV and stokesI, returning both matches and failed matches    max_separation = 4 * u.arcsec    with open(pulsarMatches +'_' + racsType + '_ASKAP_matches.table', 'rb') as pulsarMatchesDoc:        matches = pickle.load(pulsarMatchesDoc)    racsi_data = Table.read(catalog + '_' + racsType + '.hdf5')    print("Comparing stokesI and stokesV coordinates")    racsV_coords = SkyCoord(matches[0]['ra_deg_cont'], matches[0]['dec_deg_cont'], unit=('deg', 'deg'))    racsI_coords = SkyCoord(racsi_data['ra_deg_cont'], racsi_data['dec_deg_cont'], unit=('deg', 'deg'))    print("Done matching stokesI and stokesV")    imatch, d2, _ = racsV_coords.match_to_catalog_sky(racsI_coords)    new_matches = {"racsi" :racsi_data[imatch[d2 < max_separation]], "racsv": matches[0][d2 < max_separation], "ASKAP": matches[1][d2 < max_separation],                   "distanceIandV": d2[d2 < max_separation], "distanceATNFandV": matches[2][d2 < max_separation]}    failed_matches = {"racsi": racsi_data[imatch[d2 > max_separation]], "racsv": matches[0][d2 > max_separation], "ASKAP": matches[1][d2 > max_separation],                      "distanceIandV": d2[d2 > max_separation], "distanceATNFandV": matches[2][d2 < max_separation]}    new_matches["racsi"].add_column(Column(racsI_coords[imatch[d2 < max_separation]], name="skycoord"))    failed_matches["racsi"].add_column(Column(racsI_coords[imatch[d2 > max_separation]], name="skycoord"))    with open('combined_matches' + "_" + racsType + '.table', 'wb') as pulsarMatchesDoc:        pickle.dump(new_matches, pulsarMatchesDoc)    with open('failed_matches' +"_" + racsType + '.table', 'wb') as pulsarMatchesDoc:        pickle.dump(failed_matches, pulsarMatchesDoc)    plt.clf()    plt.hist(np.log10(d2.arcsec), bins=30, cumulative=True, log=True)    plt.xlabel('log10(Separation of pulsar in StokesI to StokesV) source [arcsec])')    plt.ylabel('Number of pulsars < separation')    #plt.plot(np.log10(max_separation.to(u.arcsec).value) * np.array([1, 1]),     #        plt.gca().get_ylim(), 'r--')    plt.savefig('StokesIandV_separation.png')    return new_matches, failed_matchesdef fluxDensityGraph(matches_data, is_far=""):    # Creates a graphs of each pulsars spectral energy density as measured by ATNF, RACS, and our calculated values    hasS400 = matches_data["ASKAP"]["S400"] != np.ma.core.MaskedConstant    hasS1400 = matches_data["ASKAP"]["S1400"] != np.ma.core.MaskedConstant    plt.rcParams.update({"errorbar.capsize": 2})    for i in range(len(matches_data["ASKAP"])):        S400 = matches_data["ASKAP"]["S400"][i]        S1400 = matches_data["ASKAP"]["S1400"][i]        S400_ERR = matches_data["ASKAP"]["S400_ERR"][i]        S1400_ERR = matches_data["ASKAP"]["S1400_ERR"][i]        S888Calc = matches_data["ASKAP"]["S888Calc"][i]        S888Calc_ERR = matches_data["ASKAP"]["S888_ERR"][i]        S888i = matches_data["racsi"]["flux_int"][i]        S888i_ERR = matches_data["racsi"]["flux_int_err"][i]        S888v = matches_data["racsv"]["flux_int"][i]        S888v_ERR = matches_data["racsv"]["flux_int_err"][i]        fig = plt.figure()        ax = fig.add_subplot(111)        ax.set_ylabel(r'Spectral Energy Density $\times$ MHz [mJy]')        ax.set_xlabel('Frequency [MHz]')        ax.set_title("Log Graph of Flux Density vs. Frequency for: " + matches_data["ASKAP"]["NAME"][i])        if hasS400[i] and S400 != 0.0 and hasS1400[i] and S1400 != 0:            ax.loglog([400, 1400], [S400, S1400], "o", marker="D", markersize=10)            plt.errorbar(400, S400, yerr=S400_ERR, color="black")            plt.errorbar(1400, S1400, yerr=S1400_ERR, color="black")        elif hasS400[i] and S400 != 0.0:            ax.loglog([400], [S400], "o", marker="D", markersize=10)            plt.errorbar(400, S400, yerr=S400_ERR, color="black")        elif hasS1400[i] and S1400 != 0.0:            ax.loglog([1400], [S1400], "o", marker="D", markersize=10)            plt.errorbar(1400, S1400, yerr=S1400_ERR, color="black")        ax.loglog([888], [S888Calc], "o", marker="o", markersize=10)        plt.errorbar(888, S888Calc, yerr=S888Calc_ERR, color="black")        ax.loglog([888], [S888v], "o", marker="s", markersize=10)        plt.errorbar(888, S888v, yerr=S888v_ERR, color="black")        ax.loglog([888], [S888i], "o", marker="*", markersize=15)        plt.errorbar(888, S888i, yerr=S888i_ERR, color="black")        if hasS400[i] and hasS1400[i]:            try:                x = np.array([400, 888, 1400])                scaleValue = scaleFrequency(x, "mean")                x = np.multiply(x, scaleValue)                y = np.array([S400, S888i, S1400])                yErr = np.array([S400_ERR, S888i_ERR, S1400_ERR])                spectral_index, cov = np.polyfit(np.log10(x), np.log10(y), 1, w=y/yErr, cov="unscaled")                #spectral_index, cov = curve_fit(slopeFunc, np.log10(x), np.log10(y), sigma=yErr/y)                sigma = np.sqrt(np.diag(cov))[0]                lineXVals = np.array([350, 888, 1450]) * scaleValue                lineYVals = list((j ** spectral_index[0]) * np.power(10, spectral_index[1]) for j in lineXVals)                plt.loglog(lineXVals * 1/scaleValue, lineYVals, "--", color="black")                lineYWithHighErrors = list((j ** (spectral_index[0] + sigma)) * np.power(10, spectral_index[1]) for j in lineXVals)                lineYWithLowErrors = list((j ** (spectral_index[0] - sigma)) * np.power(10, spectral_index[1]) for j in lineXVals)                plt.fill_between(lineXVals * 1/scaleValue, y1=lineYWithHighErrors, y2=lineYVals, color="0.5")                plt.fill_between(lineXVals * 1/scaleValue, y1=lineYWithLowErrors, y2=lineYVals, color="0.5")                text = r'$\alpha = {} \pm {}$'.format(str(round(np.around(spectral_index[0], 3), 2)), np.around(sigma, 2))                plt.text(0.01, 0.30, text, transform=ax.transAxes)                makeFDvsFInfo(scaleValue, x, y, yErr, lineYWithHighErrors, lineYWithLowErrors, sigma, spectral_index, cov, 'pulsars/' + matches_data["ASKAP"]["NAME"][i])            except Exception as e:                print(e)        ax.legend(["ATNF", "Calculated", "RACSV", "RACSI"], loc=3)        try:            plt.savefig('pulsars/' + matches_data["ASKAP"]["NAME"][i] + "/FDvsF" + is_far + ".png")        except:            os.mkdir("pulsars/" + matches_data["ASKAP"]["NAME"][i])            plt.savefig('pulsars/' + matches_data["ASKAP"]["NAME"][i] + "/FDvsF" + is_far + ".png")        print("Made Spectral energy density figure: " + str(i) + "/" + str(len(matches_data["racsv"])))        plt.close()def scaleFrequency(x, scaleType=False):    # Scales the frequencies used in the spectral indecies of the flux density graph    if scaleType == "mean":        return 1 / np.mean(x)    else:        return 1 / x[1]    returndef makeFDvsFInfo(scaleValue, x, y, yErr, lineYWithHighErrors, lineYwithLowErrors, sigma, spectral_index, cov, filePath):    # This creates info text on the flux density vs frequency graphs and saves it in the pulsar's folder as a text document    if not os.path.isdir(filePath):        os.mkdir(filePath)    info = ('scaleValue: {} \n' \           'x: {} \n' \           'y: {} \n' \           'y Error: {} \n' \           'y with positive sigma additional slope: {} \n' \           'y with negative sigma additional slope: {} \n' \           'Sigma: {} \n' \           'Spectral Index: {} \n' \           'Covariance Matrix: {}').format(scaleValue, x, y, yErr, lineYWithHighErrors, lineYwithLowErrors, sigma, spectral_index, cov)    try:        file = open(filePath + "/FDvsF_Info", "x")    except:        file = open(filePath + "/FDvsF_Info", "w")    file.write(info)def makeCutouts(mosaicFilePaths, pulsarData, racsType):    # uses the cutout function to zoom in on the pulsars in the RACS catalog mosaics    # Generates the names of mosaics that each pulsar is located within, and fetches the coordinates    sizes = [0.03, 0.1, 0.3, 0.9]    stokesVersions = ["i", "v"]    pos_change = -3 if racsType == "islands" else 0    for i in stokesVersions:        if i == "v":            file_names = [mosaicFilePaths[0] + j.strip()[(len(j) - (46 + pos_change)):(len(j) - (22 + pos_change))] + ".fits" for j in pulsarData["racs" + i]["fileName"]]        else:            file_names = [mosaicFilePaths[1] + j.strip()[(len(j) - (48 + pos_change)):(len(j) - (24 + pos_change))] + ".fits" for j in pulsarData["racs" + i]["fileName"]]    # Takes coordinates and file names and creates cutouts of multiple sizes.    # center of the image is ATNF, as is the blue lines.        for j in range(len(file_names)):            print("Making cutouts of " + pulsarData["ASKAP"]["NAME"][j]+ " in stokes" + i + ": " + str(j) + "/" + str(len(file_names)))            for k in sizes:                try:                    if racsType == "both":                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                      [pulsarData["racsv"][j], pulsarData["racsvIslands"][j]],                                      [pulsarData["racsi"][j],pulsarData["racsiIslands"][j]], i, racsType)                    else:                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                pulsarData["racsv"][j], pulsarData["racsi"][j], i, racsType)                except:                    os.mkdir("pulsars/" + pulsarData["ASKAP"]["NAME"][j])                    if racsType == "both":                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                      [pulsarData["racsv"][j], pulsarData["racsvIslands"][j]],                                      [pulsarData["racsi"][j],pulsarData["racsiIslands"][j]], i, racsType)                    else:                        cutout.cutOut(file_names[j], k, pulsarData["ASKAP"][j],                                pulsarData["racsv"][j], pulsarData["racsi"][j], i, racsType)def makeCutouts2(mosaicFilePaths, pulsarData):    # This takes a combined pulsar data to create and save a variety of cutouts    # Cutouts are zoomed in stokes mosaics on the coordinates of different pulsars with the different positions in catalogs marked    dataTypes = ["island", "component", "both"]    mosaicTypes = ["stokesV", "stokesI"]    sizes = [0.03, 0.1, 0.3, 0.9]    for i in range(len(pulsarData["ASKAP"])):        for j in mosaicTypes:            file_name = pulsarData["racsvIslands"]["fileName"][i].strip()[(len(pulsarData["racsvIslands"]["fileName"][i])                            - (43)):(len(pulsarData["racsvIslands"]["fileName"][i]) - (19))] + ".fits"            if j == "stokesV":                file_name = mosaicFilePaths[0] + file_name            else:                file_name = mosaicFilePaths[1] + file_name            for k in dataTypes:                markers = [pulsarData["ASKAP"][i]]                if k == "island" or k == "both":                    markers.append(pulsarData["racsiIslands"][i])                    markers.append(pulsarData["racsvIslands"][i])                elif k =="component" or k == "both":                    markers.append(pulsarData["racsi"][i])                    markers.append(pulsarData["racsv"][i])                for x in sizes:                    cutout.create_cutout(file_name, "Flux Density Vs. Position for: " +                                         pulsarData["ASKAP"]["NAME"][i] + " (" + j +")", x, markers, j, pulsarData["ASKAP"]["skycoord"][i], "total", k)        print("Created cutout for pulsar {} of {}".format(i + 1, len(failedPulsarData["ASKAP"])))def makePulsarInfo(pulsarData):    # This creates a text document of different information relating to a single pulsar and saves it in the pulsars folder    for i in range(len(pulsarData["ASKAP"])):        atnf = pulsarData["ASKAP"][i]        racsi = pulsarData["racsi"][i]        racsv = pulsarData["racsv"][i]        distanceATNFandV = pulsarData["distanceATNFandV"][i]        distanceIandV =  pulsarData["distanceIandV"][i]        try:            file = open("pulsars/" + atnf["NAME"] + "/info", "x")        except Exception as e:            file = open("pulsars/" + atnf["NAME"] + "/info", "w")        S400_ERR = str(round(float(atnf["S400_ERR"])))\            if type(atnf["S400_ERR"]) == np.float64 else "N/A"        S1400_ERR = str(round(float(atnf["S1400_ERR"])))\            if type(atnf["S1400_ERR"]) == np.float64 else "N/A"        global fileInfoText        fileInfo = fileInfoText        fileInfo.format(str(atnf["RAJD"])[:11], str(atnf["RAJD_ERR"])[:11],                   str(atnf["DECJD"])[:11], str(atnf["DECJD_ERR"])[:11],                   str(racsv["ra_deg_cont"])[:11], float(racsv["ra_err"]) * (1/3600),                   str(racsv["dec_deg_cont"])[:11], float(racsv["dec_err"]) * (1/3600),                   str(racsi["ra_deg_cont"])[:11], float(racsi["ra_err"]) * (1/3600),                   str(racsi["dec_deg_cont"])[:11], float(racsi["dec_err"]) * (1/3600),                   Angle(distanceATNFandV).dms[0],                   Angle(distanceIandV).dms[0],                   str(round(float(atnf["S400"]), 2)), S400_ERR,                   str(round(float(atnf["S888Calc"]), 2)), str(round(float(atnf["S888_ERR"]))),                   str(round(float(racsi["flux_int"]), 2)), str(round(float(racsi["flux_int_err"]), 2)),                   str(round(float(racsv["flux_int"]), 2)), str(round(float(racsv["flux_int_err"]), 2)),                   str(round(float(atnf["S1400"]), 2)), S1400_ERR,                   str(round(float(racsv["flux_int"]) / (float(racsi["flux_int"])), 5)))        file.write(fileInfo)def combineIslandsComponents(pulsarDataIslands, pulsarDataComponents):    # This returns a dictionary of tables containing information from both islands and components    matches_islands = []    matches_components = []    for i in range(len(pulsarDataIslands["ASKAP"])):        for j in range(len(pulsarDataComponents["ASKAP"])):            if pulsarDataIslands["ASKAP"]["NAME"][i] == pulsarDataComponents["ASKAP"]["NAME"][j]:                matches_islands.append(i)                matches_components.append(j)    new_pulsar_data = {"ASKAP":pulsarDataIslands["ASKAP"][matches_islands], "racsiIslands":pulsarDataIslands["racsi"][matches_islands],                       "racsi":pulsarDataComponents["racsi"][matches_components], "racsvIslands":pulsarDataIslands["racsv"][matches_islands],                       "racsv":pulsarDataComponents["racsv"][matches_components]}    return new_pulsar_datadef compareIslandsComponentsCutout(mosaicFilePaths, tablePaths, fileNameV, fileNameI):    # This creates multiple cutout of entire Stokes regions to compare Components and Islands    racsV_data = vstack(racsTableCompute(tablePaths[0],"RACS_StokesV", fileNameV))    racsI_data = vstack(racsTableCompute(tablePaths[1],"RACS_StokesI", fileNameI))    combined_data = vstack([racsV_data, racsI_data])    pos_change = -3 if racsV_data["type"][0] == "islands" else 0    file_name = racsV_data["fileName"][0].strip()[(len(racsV_data["fileName"][0])                - (43 + pos_change)):(len(racsV_data["fileName"][0]) - (19 + pos_change))] + ".fits"    cutout.create_cutout(mosaicFilePaths[0] + file_name, "StokesV Islands & Components Comparison: 0000-18A",                         25, racsV_data, "StokesV", legend="first")    cutout.create_cutout(mosaicFilePaths[1] + file_name, "StokesI Islands & Components Comparison: 0000-18A",                         25, racsI_data, "StokesI", legend="first")    cutout.create_cutout(mosaicFilePaths[1] + file_name,                         "StokesI/V Islands & Components Comparison Base StokesV: 0000-18A",                         25, combined_data, "StokesICombined", legend="first")    cutout.create_cutout(mosaicFilePaths[0] + file_name,                         "StokesI/V Islands & Components Comparison Base StokesV: 0000-18A",                         25, combined_data, "StokesVCombined", legend="first")def makeRegionFile(objects, regionName, color, markerType="circle"):    # Makes as region file for DS9 using a table of pulsars    region_info = "# Region file format: DS9 version 4.1\nglobal color=" + color + ' dashlist=8 3 width=1 font="helvetica 10 normal roman" select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\nfk5'    for i in objects:        if markerType == "circle":            region_info += "\n" + "circle(" + str(i["ra_hms_cont"]) + "," + str(i["dec_dms_cont"]) + "," + str(60) + '")'        elif markerType == "box":            region_info += "\n" + "box(" + str(i["ra_hms_cont"]) + "," + str(i["dec_dms_cont"]) + "," + str(60) + '",' + str(60) + '")'    file = open(regionName, "w")    file.write(region_info)    file.close()def createImages(pulsarData):    # Creates graphics using pil of the different cutouts, graphs, and info documents created in the rest of the code    imageSize = (600, 400)    for i in pulsarData["ASKAP"]["NAME"]:        try:            sizes = [0.03, 0.1, 0.3, 0.9]            cutOutConI = list(Image.open("pulsars/{}/cutout{},stokesI,component.png".format(i, j)).resize(imageSize) for j in sizes)            cutOutConV = list(Image.open("pulsars/{}/cutout{},stokesV,component.png".format(i, j)).resize(imageSize) for j in sizes)            #cutOutIonI = list(Image.open("pulsars/{}/cutout{}i_islands.png".format(i, j)) for j in sizes)            #cutOutIonV = list(Image.open("pulsars/{}/cutout{}v_islands.png".format(i, j)) for j in sizes)            infoImage = makeReadablePic("pulsars/{}/info".format(i), imageSize)            infoFDImage = makeReadablePic("pulsars/{}/FDvsF_Info".format(i), imageSize)            fluxDensityGraph = Image.open("pulsars/{}/FDvsF.png".format(i)).resize(imageSize)            blank = Image.new('RGB', imageSize, (255, 255, 255, 0))            image1 = cutout.createImage([cutOutConI[2], cutOutConV[2], fluxDensityGraph, infoImage], [2,2], imageSize)            cutOutPics = []            for j in range(len(sizes)):                cutOutPics.append(cutOutConI[j])                cutOutPics.append(cutOutConV[j])            image2 = cutout.createImage(cutOutPics, [2,4], imageSize)            image3 = cutout.createImage([fluxDensityGraph, infoImage, infoFDImage, blank], [2,2], imageSize)            try:                image1.save("pulsars/" + i + "/combined/main.png")                image2.save("pulsars/" + i + "/combined/cutouts.png")                image3.save("pulsars/" + i + "/combined/FDinfo.png")            except:                os.mkdir("pulsars/" + i + "/combined")                image1.save("pulsars/" + i + "/combined/main.png")                image2.save("pulsars/" + i + "/combined/cutouts.png")                image3.save("pulsars/" + i + "/combined/FDinfo.png")            print("Graphic created for pulsar: {}".format(i))        except Exception as e:            print(e)def makeReadablePic(fileName, imageSize):    #Takes a file name and image size to create an image with the text from the file on it    with open(fileName, "rb") as info:        infoImage = Image.new('RGB', (int(imageSize[0] / 1.4), int(imageSize[1] / 1.4)), (255, 255, 255, 0))        draw = ImageDraw.Draw(infoImage)        textLines = str(info.read()).split("\\n")        text = []        for j in textLines:            for k in textwrap.wrap(j, width=65):                text.append(k)        text = "\n".join(text)        text = text[2:]        text = text[:-1]        draw.text((0, 0), text, fill="black")        infoImage = infoImage.resize(imageSize)        return infoImageif len(sys.argv) != 1 and sys.argv[1] == "mortimer":    path_stokes_I_mos = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESI_IMAGES/"    path_stokes_V_mos = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESV_IMAGES/"    path_stokes_I_tables = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESI_SELAVY"    path_stokes_V_tables = str(expanduser("~")) + "/Data17/RACS/COMBINED/STOKESV_SELAVY"else:    path_stokes_I_mos = ""    path_stokes_V_mos = "mos/"    path_stokes_I_tables = "tables/racs_cati"    path_stokes_V_tables = "tables/racs_catv"if len(sys.argv) > 2 and sys.argv[2] == "redownload":    # Here it re-downloads all the data    #racsTableCompute(path_stokes_V_tables, "RACS_StokesV")    #racsTableCompute(path_stokes_I_tables, "RACS_StokesI")    getASKAP("RACS_StokesV", "components")    getASKAP("RACS_StokesV", "islands")    #getASKAP("RACS_StokesI")if len(sys.argv) > 1 and sys.argv[1] == "mortimer":    pulsarData1, failedPulsarData = compareRACSPulsars("RACS_StokesI", "RACS_StokesV", "components")    pulsarData2, failedPulsarDataIslands = compareRACSPulsars("RACS_StokesI", "RACS_StokesV", "islands")    fluxDensityGraph(pulsarData1)    #fluxDensityGraph(pulsarData2)    #makeCutouts([path_stokes_V_mos, path_stokes_I_mos], failedPulsarData, "components")    #makeCutouts([path_stokes_V_mos, path_stokes_I_mos], failedPulsarDataIslands, "islands")    #combinedData = combineIslandsComponents(pulsarData2, pulsarData1)    #makeCutouts2([path_stokes_V_mos, path_stokes_I_mos], combinedData)    #compareIslandsComponentsCutout([path_stokes_V_mos, path_stokes_I_mos],[path_stokes_V_tables, path_stokes_I_tables],    #                               "nRACS_test4_1.05_0000-18A-selavy.","RACS_test4_1.05_0000-18A.taylor.0.")    #racsV_data = racsTableCompute(path_stokes_V_tables, "RACS_StokesV", "nRACS_test4_1.05_0000-18A-selavy.")    #racsI_data = racsTableCompute(path_stokes_I_tables, "RACS_StokesI", "RACS_test4_1.05_0000-18A.taylor.0.")    #makeRegionFile(racsI_data[0], "regions/stokesIIslands","green")    #makeRegionFile(racsI_data[1], "regions/stokesIcomponents", "blue", "box")    #makeRegionFile(racsV_data[0], "regions/stokesVIslands", "red")    #makeRegionFile(racsV_data[1], "regions/stokesVcomponents", "orange", "box")    makePulsarInfo(pulsarData1)    #createImages(pulsarData1)if len(sys.argv) == 1: # For test runs on my laptop    racsTableCompute(path_stokes_V_tables, "RACS_StokesV")    getASKAP("RACS_StokesV")